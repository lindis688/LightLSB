{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":34595,"sourceType":"datasetVersion","datasetId":26922},{"sourceId":7022484,"sourceType":"datasetVersion","datasetId":4038237},{"sourceId":7047035,"sourceType":"datasetVersion","datasetId":4055140},{"sourceId":7092313,"sourceType":"datasetVersion","datasetId":4087172},{"sourceId":7092326,"sourceType":"datasetVersion","datasetId":4087182},{"sourceId":7092330,"sourceType":"datasetVersion","datasetId":4087184},{"sourceId":7092345,"sourceType":"datasetVersion","datasetId":4087194},{"sourceId":7092502,"sourceType":"datasetVersion","datasetId":4087306},{"sourceId":7095417,"sourceType":"datasetVersion","datasetId":4089230},{"sourceId":7156611,"sourceType":"datasetVersion","datasetId":4133061},{"sourceId":7201909,"sourceType":"datasetVersion","datasetId":4166007}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/openai/CLIP.git\n!pip install open_clip_torch\n!pip install sentence_transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-02T02:25:11.085720Z","iopub.execute_input":"2023-12-02T02:25:11.086660Z","iopub.status.idle":"2023-12-02T02:25:55.036791Z","shell.execute_reply.started":"2023-12-02T02:25:11.086617Z","shell.execute_reply":"2023-12-02T02:25:55.035281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import Dataset, DataLoader\nfrom fastai.vision.all import *\nimport torchvision\nimport torch\nimport torchvision.models as models\nimport torch\nimport open_clip\nimport cv2\nfrom sentence_transformers import util\nfrom PIL import Image\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom tqdm import tqdm, trange","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-02T02:25:55.039279Z","iopub.execute_input":"2023-12-02T02:25:55.039659Z","iopub.status.idle":"2023-12-02T02:26:04.586841Z","shell.execute_reply.started":"2023-12-02T02:25:55.039619Z","shell.execute_reply":"2023-12-02T02:26:04.586003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = Path(r\"/kaggle/input/lfw-yyy3-lbq\", image_size=(250, 250))\nfiles = get_image_files(path)\n\n\nclass SiameseTransform(Dataset):\n    def __init__(self, files, label_func, splits, transform=None):\n        self.labels = files.map(label_func).unique()\n        self.lbl2files = {l: L(get_image_files(path / l)) for l in self.labels}\n        self.label_func = label_func\n        self.files = files\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        f1 = self.files[idx]\n        f2, same = self._draw(f1)\n\n        # if self.transform:\n        #     img1 = self.transform(img1)\n        #     img2 = self.transform(img2)\n\n        # img1, img2 = PILImage.create(f1), PILImage.create(f2)\n        # img1 = ToTensor()(img1)  # 转换为张量\n        # img2 = ToTensor()(img2)\n        # return img1,img2,same\n        return f1, f2, same\n\n    def _draw(self, f):\n        same = random.random() < 0.5\n        cls = self.label_func(f)\n        if not same: cls = random.choice(L(l for l in self.labels if l != cls))\n        return random.choice(self.lbl2files[cls]), same","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-02T02:26:04.587976Z","iopub.execute_input":"2023-12-02T02:26:04.588536Z","iopub.status.idle":"2023-12-02T02:26:24.326034Z","shell.execute_reply.started":"2023-12-02T02:26:04.588500Z","shell.execute_reply":"2023-12-02T02:26:24.325057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image processing model\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-16-plus-240', pretrained=\"laion400m_e32\")\nmodel.to(device)\ndef imageEncoder(img):\n    img1 = Image.fromarray(img).convert('RGB')\n    img1 = preprocess(img1).unsqueeze(0).to(device)\n    img1 = model.encode_image(img1)\n    return img1\ndef generateScore(image1, image2):\n    test_img = cv2.imread(image1, cv2.IMREAD_UNCHANGED)\n    data_img = cv2.imread(image2, cv2.IMREAD_UNCHANGED)\n    img1 = imageEncoder(test_img)\n    img2 = imageEncoder(data_img)\n    cos_scores = util.pytorch_cos_sim(img1, img2)\n    score = round(float(cos_scores[0][0])*100, 2)\n    return score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-02T02:26:24.327438Z","iopub.execute_input":"2023-12-02T02:26:24.328129Z","iopub.status.idle":"2023-12-02T02:26:36.602237Z","shell.execute_reply.started":"2023-12-02T02:26:24.328099Z","shell.execute_reply":"2023-12-02T02:26:36.601242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"splits = RandomSplitter()(files)\ndataset = SiameseTransform(files, parent_label, splits)\nscores = []\nifsame = []\nlong_num = 39698\na=1\nwhile(a):\n    for images1, images2, labels in tqdm(dataset, desc=\"Processing\"):\n        # print(images1)\n        # print(images2)\n        if(len(scores)>long_num):\n            a=0\n            break\n        num = round(generateScore(str(images1), str(images2)), 2)\n        score = generateScore(str(images1), str(images2))\n        scores.append(score)\n        ifsame.append(labels)\n  # print(f\"Batch labels: {labels}\\t Score: {num}\")\nprint(f\"数据量为：{len(scores)}\")\n\n# Step 2: Calculate metrics for a range of thresholds\nbest_accuracy = 0\nbest_thresh = 0\nfor thresh in tqdm(np.arange(int(min(scores)), int(max(scores))), desc=\"Processing\"):  # assuming scores are in range 0-100\n    predictions = [s > thresh for s in scores]\n    accuracy = accuracy_score(ifsame, predictions)\n    precision = precision_score(ifsame, predictions)\n    recall = recall_score(ifsame, predictions)\n    f1 = f1_score(ifsame, predictions)\n    if accuracy > best_accuracy:\n        best_accuracy = accuracy\n        best_thresh = thresh\n        best_precision = precision\n        best_recall = recall\n        best_f1 = f1\n\n# Step 3: Print the best threshold and corresponding metrics\nprint(f\"min scores={int(min(scores))}\")\nprint(f\"min scores={int(max(scores))}\")\nprint(f\"Best threshold = {best_thresh:.3f}\")  # 保留三位小数\nprint(f\"Accuracy = {best_accuracy:.3f}\")\nprint(f\"Precision = {best_precision:.3f}\")\nprint(f\"Recall = {best_recall:.3f}\")\nprint(f\"F1_score = {best_f1:.3f}\")\n\n# print(f\"similarity Score: \", round(generateScore(r\"/kaggle/input/lfw-yuantu-2/AJ_Cook/AJ_Cook_0001.jpg\",\n#                                                  r\"/kaggle/input/lfw-yuantu-2/AJ_Lamas/AJ_Lamas_0001.jpg\"), 2))\n# similarity Score: 42.78","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-02T02:26:36.604242Z","iopub.execute_input":"2023-12-02T02:26:36.604555Z","iopub.status.idle":"2023-12-02T02:27:05.392479Z","shell.execute_reply.started":"2023-12-02T02:26:36.604528Z","shell.execute_reply":"2023-12-02T02:27:05.390860Z"},"trusted":true},"execution_count":null,"outputs":[]}]}