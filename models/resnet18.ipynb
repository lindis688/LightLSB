{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7092313,"sourceType":"datasetVersion","datasetId":4087172},{"sourceId":7092326,"sourceType":"datasetVersion","datasetId":4087182},{"sourceId":7092330,"sourceType":"datasetVersion","datasetId":4087184},{"sourceId":7092345,"sourceType":"datasetVersion","datasetId":4087194},{"sourceId":7092502,"sourceType":"datasetVersion","datasetId":4087306},{"sourceId":7095417,"sourceType":"datasetVersion","datasetId":4089230},{"sourceId":7156611,"sourceType":"datasetVersion","datasetId":4133061},{"sourceId":7201909,"sourceType":"datasetVersion","datasetId":4166007}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":3336.031214,"end_time":"2023-11-12T15:24:52.835504","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-11-12T14:29:16.804290","version":"2.4.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from fastai.vision.all import *\nimport torchvision\npath=Path(\"/kaggle/input/lfw-yyy3-lbq\")\nfiles=get_image_files(path)","metadata":{"_cell_guid":"3619ebc2-c53b-4279-a142-3d9b8d808ab6","_uuid":"78214bea-c53b-43fa-9538-a6159bb5b305","papermill":{"duration":54.553616,"end_time":"2023-11-12T14:30:14.718493","exception":false,"start_time":"2023-11-12T14:29:20.164877","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SiameseImage(fastuple):\n    def show(self, ctx=None, **kwargs): \n        img1,img2,same_breed = self\n        if not isinstance(img1, Tensor):\n            if img2.size != img1.size: img2 = img2.resize(img1.size)\n            t1,t2 = tensor(img1),tensor(img2)\n            t1,t2 = t1.permute(2,0,1),t2.permute(2,0,1)\n        else: t1,t2 = img1,img2\n        line = t1.new_zeros(t1.shape[0], t1.shape[1], 10)\n        return show_image(torch.cat([t1,line,t2], dim=2), \n                          title=same_breed, ctx=ctx)\nclass SiameseTransform(Transform):\n    def __init__(self, files, label_func, splits):\n        self.labels = files.map(label_func).unique()\n        self.lbl2files = {l: L(get_image_files(path/l)) for l in self.labels}\n        self.label_func = label_func\n        self.valid = {f: self._draw(f) for f in files[splits[1]]}\n        \n    def encodes(self, f):\n        f2,t = self.valid.get(f, self._draw(f))\n        img1,img2 = PILImage.create(f),PILImage.create(f2)\n        return SiameseImage(img1, img2, t)\n    \n    def _draw(self, f):\n        same = random.random() < 0.5\n        cls = self.label_func(f)\n        if not same: cls = random.choice(L(l for l in self.labels if l != cls)) \n        return random.choice(self.lbl2files[cls]),same\n    \nsplits = RandomSplitter()(files)\ntfm = SiameseTransform(files, parent_label, splits)\ntls = TfmdLists(files, tfm, splits=splits)\n\ndef halftone(img):\n    if isinstance(img, PILImage):\n        img1=img.convert(\"1\").convert(\"L\")\n        return img1\n    else:\n        return img\n\ndls = tls.dataloaders(after_item=[Resize(224), ToTensor], \n    after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)])\n#*aug_transforms()\nclass SiameseModel(Module):\n    def __init__(self, encoder, head):\n        self.encoder,self.head = encoder,head\n    def forward(self, x1, x2):\n        x1 = self.encoder(x1)\n        x2 = self.encoder(x2)\n        x1 = self.head(x1)\n        x2 = self.head(x2)\n        return x1, x2","metadata":{"_cell_guid":"7298ca9b-41b3-496d-b6f1-06c42ee2e524","_uuid":"77d7933f-29ef-469b-8a01-a81cfadadbd9","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":8.769438,"end_time":"2023-11-12T14:30:23.492339","exception":false,"start_time":"2023-11-12T14:30:14.722901","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@typedispatch\ndef show_batch(x:SiameseImage, y, samples, ctxs=None, max_n=6, nrows=None, ncols=2, figsize=None, **kwargs):\n    if figsize is None: figsize = (ncols*6, max_n//ncols * 3)\n    if ctxs is None: ctxs = get_grid(min(x[0].shape[0], max_n), nrows=None, ncols=ncols, figsize=figsize)\n    for i,ctx in enumerate(ctxs): SiameseImage(x[0][i], x[1][i], ['Not similar','Similar'][x[2][i].item()]).show(ctx=ctx)","metadata":{"papermill":{"duration":0.013708,"end_time":"2023-11-12T14:30:23.510343","exception":false,"start_time":"2023-11-12T14:30:23.496635","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls.show_batch()","metadata":{"papermill":{"duration":2.579959,"end_time":"2023-11-12T14:30:26.094066","exception":false,"start_time":"2023-11-12T14:30:23.514107","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nencoder = create_body(resnet18(pretrained=True), cut=-2)\nhead = create_head(512, 128, [512,256], 0.5, bn_final=True)\nmodel = SiameseModel(encoder, head)","metadata":{"papermill":{"duration":1.183193,"end_time":"2023-11-12T14:30:27.290937","exception":false,"start_time":"2023-11-12T14:30:26.107744","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastai.optimizer import Adam\nclass ContrastiveLoss(nn.Module):\n    def __init__(self, margin=1.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, ops, target, size_average=True):\n        op1, op2 = ops[0], ops[1]\n        dist = F.pairwise_distance(op1, op2)\n        pdist = dist * target\n        ndist = dist * (~target)\n        loss = 0.5 * ((pdist**2) + (F.relu(self.margin - ndist)**2))\n        loss = loss.mean() if size_average else loss.sum()\n        return loss\n\ndef siamese_splitter(model):\n    return [params(model.encoder), params(model.head)]\n           \n\ndef contrastive_accuracy(ops, targets):\n    op1, op2 = ops[0], ops[1]\n    dists = F.pairwise_distance(op1, op2)\n    optimal_threshold.find(dists, targets)\n    preds = (dists < optimal_threshold.best_threshold).float()\n    return (preds == targets).float().mean()\n\ndef accuracy(ops, targets):\n    op1, op2 = ops[0], ops[1]\n    dists = F.pairwise_distance(op1, op2)\n    optimal_threshold.find(dists, targets)\n    preds = (dists < optimal_threshold.best_threshold)\n\n    true_positives = torch.sum((preds == 1) & (targets == 1)).item()\n    true_negatives = torch.sum((preds == 0) & (targets == 0)).item()\n    false_positives = torch.sum((preds == 1) & (targets == 0)).item()\n    false_negatives = torch.sum((preds == 0) & (targets == 1)).item()\n\n    accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives) if (true_positives + true_negatives + false_positives + false_negatives) > 0 else 0.0\n\n    return accuracy\n\ndef precision(ops, targets):\n    op1, op2 = ops[0], ops[1]\n    dists = F.pairwise_distance(op1, op2)\n    optimal_threshold.find(dists, targets)\n    preds = (dists < optimal_threshold.best_threshold)\n\n    true_positives = torch.sum((preds == 1) & (targets == 1)).item()\n    false_positives = torch.sum((preds == 1) & (targets == 0)).item()\n\n    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0\n\n    return precision\n\ndef recall(ops, targets):\n    op1, op2 = ops[0], ops[1]\n    dists = F.pairwise_distance(op1, op2)\n    optimal_threshold.find(dists, targets)\n    preds = (dists < optimal_threshold.best_threshold)\n\n    true_positives = torch.sum((preds == 1) & (targets == 1)).item()\n    false_negatives = torch.sum((preds == 0) & (targets == 1)).item()\n\n    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0\n\n    return recall\n\ndef f1_score(ops, targets):\n    op1, op2 = ops[0], ops[1]\n    dists = F.pairwise_distance(op1, op2)\n    optimal_threshold.find(dists, targets)\n    preds = (dists < optimal_threshold.best_threshold)\n\n    true_positives = torch.sum((preds == 1) & (targets == 1)).item()\n    false_positives = torch.sum((preds == 1) & (targets == 0)).item()\n    false_negatives = torch.sum((preds == 0) & (targets == 1)).item()\n\n    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0\n    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0\n\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n\n    return f1_score\n\nclass OptimalThreshold:\n    def __init__(self, start=0.1, end=4.0, step=0.01):\n        self.start = start\n        self.end = end\n        self.step = step\n        self.best_threshold = None\n\n    def find(self, preds, targets):\n        best_accuracy = 0.0\n        for threshold in np.arange(self.start, self.end, self.step):\n            preds_thresholded = (preds < threshold).float()\n            accuracy = (preds_thresholded == targets).float().mean()\n            if accuracy > best_accuracy:\n                best_accuracy = accuracy\n                self.best_threshold = threshold\n                \nclass PrintBestThreshold(Callback):\n    def after_epoch(self):\n        print(f\"Epoch {self.learn.epoch} LR:{self.opt.hypers[0]['lr']:.10f} Best threshold: {optimal_threshold.best_threshold:.2f}\")\n\noptimal_threshold = OptimalThreshold()\n\nlearn = Learner(dls, model, loss_func=ContrastiveLoss(margin=5.0), opt_func=Adam,\n                splitter=siamese_splitter, metrics=[accuracy, precision, recall, f1_score],cbs=PrintBestThreshold())","metadata":{"_cell_guid":"f2c2b1e8-1e1d-4284-9ddb-3ed967c22b09","_uuid":"a058bf10-508b-4a08-9d94-d48c5c1cf4f5","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.044372,"end_time":"2023-11-12T14:30:27.351615","exception":false,"start_time":"2023-11-12T14:30:27.307243","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.lr_find()","metadata":{"_cell_guid":"bb3851bb-468d-4a92-bdbc-2c20bba58ec8","_uuid":"d0fb1b11-87b2-4f06-b81b-dbaa6aecbe41","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":71.001243,"end_time":"2023-11-12T14:31:38.369248","exception":false,"start_time":"2023-11-12T14:30:27.368005","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.freeze()\nlearn.fit_one_cycle(15, 1e-3)","metadata":{"_cell_guid":"06274d34-2207-4f2c-bedb-51f5978e3d09","_uuid":"a8f4db32-a868-46b3-b051-94694e75f383","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":1409.093015,"end_time":"2023-11-12T14:55:07.479852","exception":false,"start_time":"2023-11-12T14:31:38.386837","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.unfreeze()\nlearn.fit_one_cycle(15, slice(1e-5,1e-3))","metadata":{"papermill":{"duration":1782.148118,"end_time":"2023-11-12T15:24:49.647164","exception":false,"start_time":"2023-11-12T14:55:07.499046","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.save('ResNet18_lfw_yyy3_lbq')","metadata":{"papermill":{"duration":0.548374,"end_time":"2023-11-12T15:24:50.216224","exception":false,"start_time":"2023-11-12T15:24:49.667850","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}